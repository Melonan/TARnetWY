{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# from aeon.datasets.tsc_data_lists import multivariate_equal_length\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time \n",
    "# from utils import geng\n",
    "import logconfig\n",
    "import geng\n",
    "import torch\n",
    "import logging\n",
    "# # 修改一个已存在的环境变量\n",
    "# os.environ['PATH'] = '/usr/local/cuda-11.6/bin:' + os.environ['PATH']\n",
    "# os.environ['LIBRARY_PATH'] = '/usr/local/cuda-11.6/lib64:' + os.environ['LIBRARY_PATH']\n",
    "# os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda-11.6/lib64:'+ os.environ['LD_LIBRARY_PATH']\n",
    "# 使用设置好的环境变量\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"7\"\n",
    "logconfig.setup_logging(dir=\"./\")\n",
    "logging.info(f\"PATH:{os.environ['PATH']}\\nCUDA_HOME:{os.environ['CUDA_HOME']}\")\n",
    "# dataset_name = \"170Kailuan-relu-5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from aeon.datasets import load_classification\n",
    "# import numpy as np\n",
    "# dataset_name = \"Libras\"\n",
    "# archive_name=\"MTS\"\n",
    "# X, Y,meta_data = load_classification(dataset_name,return_metadata=True)\n",
    "# X_reshaped = np.transpose(X, (0, 2, 1))\n",
    "# X = X_reshaped\n",
    "# nb_classes = len(np.unique(Y, axis=0))\n",
    "# nb_classes,X.shape,Y.shape,np.unique(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Kailuan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "archive_name=\"MTS\"\n",
    "time_series1 = pd.read_csv(\"./combined_sheets.csv\")\n",
    "time_series2 = pd.read_csv(\"./combined_sheets2.csv\")\n",
    "meta_data = pd.read_csv(\"./combinde170_info.csv\")\n",
    "time_series1.shape,time_series2.shape,meta_data.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series2 = time_series2.drop(['SystolicPressure','DiastolicPressure'],axis=1)\n",
    "time_series = pd.concat([time_series1, time_series2], axis=0)\n",
    "time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movement全变成1 二值化\n",
    "# time_series['Movement'] = np.where(time_series['Movement'] != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构造每个source的二维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = time_series.groupby('Source')\n",
    "\n",
    "# 提取特征列\n",
    "features = [ 'Breath', 'Heartrate']\n",
    "\n",
    "# # 将每个组转换为二维数组并堆叠\n",
    "# three_dim_array = np.array([group[features].values for _, group in grouped])\n",
    "\n",
    "# three_dim_array.shape # 显示三维数组的形状"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movement 处理\n",
    "- Movement 取消： 只有fold3有提升 其他4个fold下降\n",
    "- Movement 全部+1 不行 都有下降\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_groupby = time_series.groupby('Source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看movement的分布\n",
    "# movement_stac=[{}]\n",
    "# for i in range(1,136):\n",
    "#     logging.info(i)\n",
    "#     keys, vals = np.unique(time_groupby.get_group(int(i))[\"Movement\"].values,return_counts=True)\n",
    "#     result_dict = dict(zip(keys, vals))\n",
    "#     logging.info(result_dict)\n",
    "#     movement_stac.append(result_dict)\n",
    "\n",
    "# # movement_stac"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充 截断 时间序列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充/截断 构造数据集(去掉过少的)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial block \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新计算每个样例的时间步数（考虑到了数据过滤）\n",
    "filtered_time_steps_per_source = time_series.groupby('Source').size()\n",
    "\n",
    "# 重新筛选出有效的 Source 过滤掉过少的\n",
    "valid_sources = filtered_time_steps_per_source[filtered_time_steps_per_source > 500].index\n",
    "obsolete_sources = filtered_time_steps_per_source[filtered_time_steps_per_source <= 500].index\n",
    "logging.info(\"valid sources:\", valid_sources, \"\\nobsolete sources:\", obsolete_sources, \"\\n len of obsoletes:\",filtered_time_steps_per_source[obsolete_sources])\n",
    "# 重新获取有效的数据\n",
    "valid_filtered_time_series = time_series[time_series['Source'].isin(valid_sources)]\n",
    "\n",
    "# 重新分组\n",
    "valid_filtered_grouped = valid_filtered_time_series.groupby('Source')\n",
    "\n",
    "# 重新计算平均时间步数（此时应该没有 NaN 值）\n",
    "average_time_steps = int(valid_filtered_grouped.size().mean())\n",
    "\n",
    "logging.info(f\"\\n average_time_steps:{average_time_steps}\")\n",
    "\n",
    "# 初始化新的三维数组（考虑到了有效的样例数）\n",
    "valid_sources_count = len(valid_sources)\n",
    "three_dim_array_valid = np.zeros((valid_sources_count, average_time_steps, len(features)))\n",
    "\n",
    "# 填充新的三维数组\n",
    "for i, (source, group) in enumerate(valid_filtered_grouped):\n",
    "    data = group[features].values\n",
    "    current_steps = data.shape[0]\n",
    "    if current_steps < average_time_steps:\n",
    "        fill_values = {\n",
    "            'Breath': group['Breath'].mean(),\n",
    "            'Heartrate': group['Heartrate'].mean(),\n",
    "        }\n",
    "        fill_array = np.array([[fill_values[feature] for feature in features]] * (average_time_steps - current_steps))\n",
    "        full_data = np.vstack(( fill_array,data))\n",
    "    else:\n",
    "        # 调整为截取较后面的元素\n",
    "        full_data = data[current_steps-average_time_steps:, :]\n",
    "    # logging.info(f\"source:{source},current_steps:{current_steps}, full_data shape:{full_data.shape}\")\n",
    "    three_dim_array_valid[i, :, :] = full_data\n",
    "\n",
    "\n",
    "final_data = three_dim_array_valid\n",
    "\n",
    "# 进行数据标准化\n",
    "final_data = geng.standard_scaler_total(final_data)\n",
    "\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签构造\n",
    "- 意识状态\n",
    "- GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = meta_data[\"意识状态\"].values\n",
    "\n",
    "valid_sources = valid_sources-1\n",
    "labels = [labels[i] for i in range(len(labels)) if i in valid_sources]\n",
    "# labels = [1 if x == 2 else x for x in labels]\n",
    "nb_classes = len(np.unique(labels, axis=0))\n",
    "labels = np.array(labels)\n",
    "labels, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (填充/截断)构造数据集 原本版 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# average_time_steps = int(grouped.apply(lambda x: x.shape[0]).mean())\n",
    "\n",
    "# # 初始化三维数组\n",
    "# features = ['Sleepstate', 'Breath', 'Heartrate', 'Movement']\n",
    "# three_dim_array = np.zeros((135, average_time_steps, len(features)))\n",
    "\n",
    "# # 填充三维数组\n",
    "# for source, group in grouped:\n",
    "#     data = group[features].values\n",
    "#     current_steps = data.shape[0]\n",
    "\n",
    "#     if current_steps < average_time_steps:\n",
    "#         fill_values = {\n",
    "#             'Breath': group['Breath'].mean(),\n",
    "#             'Heartrate': group['Heartrate'].mean(),\n",
    "#             'Sleepstate': group['Sleepstate'].median(),\n",
    "#             'Movement': 0\n",
    "#         }\n",
    "#         fill_array = np.array([[fill_values[feature] for feature in features]] * (average_time_steps - current_steps))\n",
    "#         full_data = np.vstack((data, fill_array))\n",
    "#     else:\n",
    "#         full_data = data[:average_time_steps, :]\n",
    "\n",
    "#     three_dim_array[source-1, :, :] = full_data\n",
    "\n",
    "# three_dim_array.shape  # 显示三维数组的形状\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签构造\n",
    "- 意识状态\n",
    "- GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = meta_data[\"意识状态\"].values\n",
    "# nb_classes = len(np.unique(labels, axis=0))\n",
    "# labels,nb_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_final_process(x,y):\n",
    "    nb_classes = len(np.unique(y, axis=0))\n",
    "    # transform the labels from integers to one hot vectors\n",
    "    enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
    "    enc.fit(y.reshape(-1, 1))\n",
    "    y = enc.transform(y.reshape(-1, 1)).toarray()\n",
    "\n",
    "    # # save orignal y because later we will use binary\n",
    "    # y_true = np.argmax(y_test, axis=1)\n",
    "    logging.info(f'x.shape: {x.shape}, y.shape: {y.shape}\\n')\n",
    "    if len(x.shape) == 2:  # if univariate\n",
    "        # add a dimension to make it multivariate with one dimension \n",
    "        x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "    input_shape = x.shape[1:]\n",
    "    logging.info(f'input_shape: {input_shape}\\n')\n",
    "    \n",
    "    return x,y,input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_name=\"fcn\"\n",
    "# root_dir=\"/data4/gsprivate/dl-4-tsc\"\n",
    "# output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '' + '/' + \\\n",
    "#                     dataset_name + '/'\n",
    "                    \n",
    "# if create_directory(output_directory) is None:\n",
    "#     logging.info(\"Creating directory:{} None\".format(output_directory))\n",
    "\n",
    "# x,y,input_shape=data_final_process(final_data,labels)\n",
    "# fcn_classifier = geng.create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
    "# logging.info(f\"Classifier x.shape={x.shape} ,y.shape={y.shape}, input_shape={input_shape}, nb_classes={nb_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier_name=\"cnn\"\n",
    "# root_dir=\"/data4/gsprivate/dl-4-tsc\"\n",
    "# output_directory = root_dir + '/results/' + classifier_name + '/' + archive_name + '' + '/' + \\\n",
    "#                     dataset_name + '/'\n",
    "                    \n",
    "# if create_directory(output_directory) is None:\n",
    "#     logging.info(\"Creating directory:{} None\".format(output_directory))\n",
    "\n",
    "# x,y,input_shape=data_final_process(final_data,labels)\n",
    "# cnn_classifier = geng.create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
    "# logging.info(f\"Classifier x.shape={x.shape} ,y.shape={y.shape}, input_shape={input_shape}, nb_classes={nb_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'Kailuan'\n",
    "        self.batch = 16\n",
    "        self.lr = 0.01\n",
    "        self.nlayers = 2\n",
    "        self.emb_size = 64\n",
    "        self.nhead = 8\n",
    "        self.task_rate = 0.5\n",
    "        self.masking_ratio = 0.15\n",
    "        self.lamb = 0.8\n",
    "        self.epochs = 300\n",
    "        self.ratio_highest_attention = 0.5\n",
    "        self.avg = 'macro'\n",
    "        self.dropout = 0.01\n",
    "        self.nhid = 128\n",
    "        self.nhid_task = 128\n",
    "        self.nhid_tar = 128\n",
    "        self.task_type = 'classification'  # Or 'regression', depending on your task\n",
    "\n",
    "# 使用这个 Args 类来创建一个 args 对象\n",
    "args = Args()\n",
    "\n",
    "# 现在你可以像在命令行环境中一样使用这些参数了\n",
    "# 例如，打印学习率\n",
    "logging.info(args.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = utils.get_prop(args)\n",
    "logging.info(\"prop: \" , prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name=\"TARnet\"\n",
    "root_dir=\"/data4/gsprivate/TARNet/\"\n",
    "output_directory = root_dir + '/results/' + classifier_name + '/'\n",
    "        \n",
    "                    \n",
    "if geng.create_directory(output_directory) is None:\n",
    "    logging.info(\"Creating directory:{} None\".format(output_directory))\n",
    "\n",
    "# x,y,input_shape=data_final_process(final_data,labels)\n",
    "# mcnn_classifier = geng.create_classifier(classifier_name, input_shape, nb_classes, output_directory)\n",
    "k_splits = geng.splits(final_data,labels,4)\n",
    "idx = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "for x_train_fold, y_train_fold, x_val_fold, y_val_fold in k_splits:\n",
    "    logging.info(f\"\\n---fold:{idx}---\")\n",
    "    logging.info(f\"x_train_fold.shape:{x_train_fold.shape}, y_train_fold.shape:{y_train_fold.shape}, x_val_fold.shape:{x_val_fold.shape}, y_val_fold.shape:{y_val_fold.shape}\")\n",
    "    \n",
    "    X_train_task, y_train_task, X_test, y_test = utils.preprocess(prop, x_train_fold, y_train_fold, x_val_fold, y_val_fold)\n",
    "    logging.info(f\"After preprocess : X_train_task.shape:{X_train_task.shape}, y_train_task.shape:{y_train_task.shape}, X_test.shape:{ X_test.shape}, y_test.shape:{y_test.shape}\")\n",
    "    \n",
    "    prop['nclasses'] = torch.max(y_train_task).item() + 1 if prop['task_type'] == 'classification' else None\n",
    "    prop['dataset'], prop['seq_len'], prop['input_size'] = prop['dataset'], X_train_task.shape[1], X_train_task.shape[2]\n",
    "    prop['device'] = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "    logging.info(f\"prop:{prop}\")\n",
    "\n",
    "    idx+=1\n",
    "    logging.info('Initializing model...')\n",
    "    model, optimizer, criterion_tar, criterion_task, best_model, best_optimizer = utils.initialize_training(prop)\n",
    "    logging.info('Model intialized...')\n",
    "\n",
    "    logging.info('Training start...')\n",
    "    utils.training(model, optimizer, criterion_tar, criterion_task, best_model, best_optimizer, X_train_task, y_train_task, X_test, y_test, prop)\n",
    "    logging.info('Training complete...')\n",
    "\n",
    "# logging.info(f\"Classifier x.shape={x.shape} ,y.shape={y.shape}, input_shape={input_shape}, nb_classes={nb_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture cap --no-stderr\n",
    "# logging.info(\"这将写入到文件中\")\n",
    "\n",
    "# geng.fit_splits_for_mcnn(mcnn_classifier,x,y,epochs=250)\n",
    "logging.info(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geng.fit_splits(cnn_classifier,x,y,batch_size=8,epochs=200)\n",
    "# logging.info('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch \n",
    "import tensorflow as tf\n",
    "\n",
    "# logging.info(torch.version.cuda)·\n",
    "# logging.info(torch.backends.cudnn.version())\n",
    "logging.info(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Retrieving all environment variables\n",
    "# env_vars = os.environ\n",
    "\n",
    "# # Displaying the environment variables\n",
    "# env_vars_dict = {key: env_vars[key] for key in env_vars}\n",
    "# env_vars_dict['LD_LIBRARY_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsprivate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
